---
title: "MORSE tutorial"
date: "`r Sys.Date()`"
output: rmarkdown::html_vignette
self_contained: no
vignette: >
  %\VignetteIndexEntry{Tutorial}
  %\VignetteEngine{knitr::rmarkdown}
  \usepackage[utf8]{inputenc}
  
---

```{r,include=FALSE, echo=FALSE}

knitr::opts_chunk$set(fig.width = 7, 
                      fig.height = 4, 
                      cache = TRUE)

```


```{r, echo=FALSE, cache=TRUE, results='hide'}
library(morse)
library(ggplot2)
```

The package MORSE is devoted to the analysis of data from standard toxicity
bioassays. It provides a simple workflow to explore/visualize a dataset, and
compute estimations of risk assessment indicators. This document illustrates
a typical use of MORSE on survival and reproduction data, which can be followed
to analyze new datasets.

# Survival data analysis

The following example shows all the stages to perform survival analysis on
standard bioassay data and produce estimate values of the $LC_x$.
We will use a dataset of the library named `cadmium2`, which contains both
survival and reproduction data from a chronic laboratory bioassay. In this
experiment, snails were exposed to six concentrations of a metal contaminant
(cadmium) during 56 days.

## Step 1: check the structure and the dataset integrity

The data from a survival assay should be gathered in a `data.frame` with a 
specific layout. This is documented in the paragraph on `survData` in the 
reference manual, and you can also inspect one of the datasets provided in 
the package (like `cadmium2`). First, we load the dataset and use the function
`survDataCheck` to verify that it has the expected layout:

```{r, cache=TRUE}
data(cadmium2)
survDataCheck(cadmium2)
```

## Step 2: create a `survData` object

The class `survData` represents \emph{validated} survival data and is the 
basic representation used for the subsequent operations. Note that if the
call to `survDataCheck` reports no error, it is guaranteed that `survData`
will not fail.

```{r, cache=TRUE}
dat <- survData(cadmium2)
head(dat)
```

## Step 3: visualize your dataset
  
The function `plot` can be used to plot the number of surviving individuals
as a function of time for all concentrations and replicates.

```{r, cache=TRUE}
plot(dat, style = "ggplot", pool.replicate = FALSE)
```

Two graphical styles are available, `"generic"` for standar `R` plots or
`"ggplot"` to call the package `ggplot2`. If the argument `pool.replicate`
is `TRUE` the datapoints for a given time and concentration are pooled
and only the mean number of survivors is plotted. To observe the full
dataset, we set this option to `FALSE`.

By fixing the concentration to a (tested) value, we can visualize one subplot 
in particular:
```{r, cache=TRUE}
plot(dat, concentration = 124, addlegend = TRUE,
     pool.replicate = FALSE, style = "ggplot")
```

We can also plot the survival rate with the confidence intervale on the data
at a given time point as a function of the concentration, by using the function
`plot.DoseResponse` and by fixing `target.time`:
  
```{r, cache=TRUE}
plotDoseResponse(dat, target.time = 21, style = "ggplot", addlegend = TRUE)
```

The function `summary` provides some descriptive statistics on the experimental design.

```{r, cache=TRUE}
summary(dat)
```

## Step 4: fit an exposure-response model to the survival data at target time

Now we are ready to fit a probabilistic model for the survival data, which aims
at finding the relation between concentration of pollutant and mean survival rate
at the target time. Our model assumes the latter is a log-logistic function of
the former, and the work here is to estimate the parameters of this log-logistic
function. Once we have estimated the parameters, we can then calculate the $LC_x$
for any $x$. All this work is performed by the `survFitTT` function, which requires
a `survData` object and the levels of $LC_x$ we need:

```{r, results="hide", cache=TRUE}
fit <- survFitTT(dat,
                 target.time = 21,
                 lcx = c(10, 20, 30, 40, 50))
```

The returned value is an object of class `survFitTT` and provides the estimated 
parameters as a posterior[^1] distribution, in order to model our uncertainty on
their true value. For the parameters of the models, as well as the $LC_x$
values, we report the median (as the estimated value) and the 2.5 \% and 97.5 \%
quantiles of the posterior (as a measure of uncertainty, a.k.a. credible
intervals). These can seen using the `summary` method:


```{r, cache=TRUE}
summary(fit)
```

If the inference went well, it is expected that the difference between 
quantiles in the posterior will be reduced compared to the prior, meaning
that the data was helpful to reduce the uncertainty on the true value of
the parameters. This simple check can be performed using the summary function.

The fit can also be plotted:
```{r, cache=TRUE}
plot(fit, log.scale = TRUE, style = "ggplot", adddata = TRUE,
     addlegend = TRUE)
```
This representation shows the estimated relation between concentration of
pollutant and survival rate (red curve). It is computed by choosing for each
parameter the median value of its posterior. To assess the uncertainty on this
estimation, we compute many such curves this time by sampling the parameters 
in the posterior distribution. This gives rise to the pink band, showing for
any given concentration an interval (called credible interval) containing the
survival rate 95% of the time in the posterior distribution. The experimental
data points are represented in black and correspond to the observed survival
rate when
pooling all replicates. The black error bars correspond to a 95% confidence
interval, which is another, more straightforward way to bound the most
probable value of the survival rate for a tested concentration. In favorable
situations, we expect that the credible interval around the estimated curve
and the confidence interval around experimental points will largely overlap.

Note that `survFitTT` will warn you if the estimated $LC_{50}$ lies outside the
range of tested concentrations, as in the following example:

```{r, results="hide", cache=TRUE}
data("cadmium1")
wrong_fit <- survFitTT(survData(cadmium1),
                       target.time = 21,
                       lcx = c(10, 20, 30, 40, 50))
plot(wrong_fit, log.scale = TRUE, style = "ggplot", adddata = TRUE,
     addlegend = TRUE)
```

In this example, the experimental design did not include sufficiently high
concentrations, and we are missing measurements that would have a major 
influence on the final estimation. For this reason this result should be 
considered unreliable.



## Step 5: validate the model with a posterior predictive check

The estimation can be further validated using so-called posterior
predictive checks: the idea is to plot the observed values against the
corresponding estimated predictions, along with their 95% credible
interval. If the fit is correct, we expect to see 95% of the data
inside the intervals.

```{r,cache=TRUE, results="hide"}
ppc(fit, style = "ggplot")
```

In this plot, each black dot represents an observation made at some
concentration, and the number of survivors at target time is given by
the value on X-axis. Using the concentration and the fitted model, we
can produce a corresponding prediction of the expected number of
survivors at that concentration. This prediction is given by the
Y-axis. Ideally observations and predictions should coincide, so we'd
expect to see the black dots on the points of coordinate $Y = X$. Our
model provides a tolerable variation around the predited mean value
as an interval where we expect 95% of the dots to be in average. The
intervals are represented in green if they overlap with the line $Y=X$,
and in red otherwise.
As replicates are shifted on the x-axis, the bisecting line (y = x), is
represented by steps, and is added to the plot in order to see if each
prediction interval contains each observed value. 

# Reproduction data analyses
  
The steps in reproduction data analysis are absolutely analogous to what we
described for survival data. This time the goal is to estimate the relation 
between pollutant concentration and reproduction rate per individual-day. 

Here is a typical session:
```{r, cache=TRUE}
# (1) load dataset
data(cadmium2)

# (2) check structure and integrity of the dataset
reproDataCheck(cadmium2)

# (3) create a `reproData` object
dat <- reproData(cadmium2)

# (4) represent the cumulated number of offspring as a function time
plot(dat, style = "ggplot", pool.replicate = FALSE)
plot(dat, concentration = 124, addlegend = TRUE, style = "ggplot",
     pool.replicate = FALSE)

# (5) represent the reproduction rate as a function of concentration
plotDoseResponse(dat, target.time = 56, style = "ggplot")

# (6) check information on the experimental design
summary(dat)

# (7) fit an exposure-response model at target-time
fit <- reproFitTT(dat, stoc.part = "bestfit",
                  target.time = 21,
                  ecx = c(10, 20, 30, 40, 50),
                  quiet = TRUE)
summary(fit)
```

```{r, cache=TRUE}
plot(fit, log.scale = TRUE, style = "ggplot", adddata = TRUE,
     addlegend = TRUE)
```

```{r, cache=TRUE}
ppc(fit, style = "ggplot")
```
As in survival analysis, we assume that the reproduction rate per individual-day
is a log-logistic function of the concentration. More details and parameter
signification can be found in the Modeling vignette. 


## Model comparison

For reproduction analysis, we consider one model which neglects inter-individual
variability (named "Poisson") and another one which takes it into account
(named "gamma Poisson"). You can choose either one using the option `stoc.part`,
but by setting it to `"bestfit"`, you let `reproFitTT` decide which models fits the
data best. The choice can be seen by calling the `summary` function:
```{r, cache=TRUE}
summary(fit)
```
When the gamma Poisson model is selected, the summary shows an additional
parameter called `omega`, which quantifies the inter-individual variability
(the higher `omega` the higher the variability).


## Reproduction data and survival functions

In MORSE, reproduction datasets are a special case of survival datasets: a 
reproduction dataset includes the same information than a survival dataset plus
the information on reproduction outputs. For that reason the S3 class `reproData`
inherits from the class `survData`, which means that any operation on a `survData`
object is legal on a `reproData` object. In particular, to use the plot function
related to survival analysis on `reproData` object, we can use the `as.survData`
conversion function:

```{r, cache=TRUE}
dat <- reproData(cadmium2)
plot(as.survData(dat))
```

# Toxico-kinetic toxico-dynamic data analyses
  
The steps in TKTD data analysis are absolutely analogous to what we
described for survival data. This time the goal is to estimate the relation 
between pollutant concentration, time and survival rate. 

Here is a typical session:
```{r, cache=TRUE}
# (1) load dataset
data(propiconazole)

# (2) check structure and integrity of the dataset
survDataCheck(propiconazole)

# (3) create a `survData` object
dat <- survData(propiconazole)

# (4) represent the number of survivor as a function time
plot(dat, style = "ggplot", pool.replicate = FALSE)

# (5) check information on the experimental design
summary(dat)

# (6) fit an time-exposure-response model at
fit <- survFitTKTD(dat, quiet = TRUE)
summary(fit)
```

```{r, cache=TRUE}
plot(fit, style = "ggplot", adddata = TRUE, one.plot = FALSE)
```

```{r, cache=TRUE}
ppc(fit, style = "ggplot")
```

[^1]: In Bayesian inference, the parameters of a model are estimated
from the data starting from a so-called *prior*, which is a probability
distribution representing an initial guess on the true parameters, before
seing the data. The *posterior* distribution represents the uncertainty on
the parameters after seeing the data and combining it with the prior. To obtain
a point estimate of the parameters, it is typical to compute the mean or median
of the posterior and quantify the uncertainty by reporting the standard
deviation or inter-quantile distance.

